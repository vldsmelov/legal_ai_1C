name: legal_ai_1c

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    healthcheck:
      # healthcheck тоже через API, не через CLI
      test: ["CMD-SHELL", "OLLAMA_HOST=127.0.0.1:11434 ollama list >/dev/null 2>&1 || exit 1"]      
      interval: 10s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  ollama-pull:
    image: curlimages/curl:8.10.1
    container_name: ollama-pull
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_API=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL}
    # Ждём API и вызываем /api/pull — модель скачает САМ демон ollama (в его volume)
    entrypoint:
      - /bin/sh
      - -lc
      - |
        echo "[pull] waiting for $${OLLAMA_API} ..."
        for i in $$(seq 1 120); do
          if curl -fsS $${OLLAMA_API}/api/tags >/dev/null; then
            echo "[pull] server is up"; break
          fi
          sleep 2
        done
        echo "[pull] start pull: $${OLLAMA_MODEL}"
        curl -fsS -X POST $${OLLAMA_API}/api/pull \
          -H 'Content-Type: application/json' \
          -d '{"name":"'"$${OLLAMA_MODEL}"'"}' \
          | sed -u 's/^/[pull] /'
        echo "[pull] done"
    restart: "no"


  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

  backend:
    image: legal-ai/backend:dev
    build:
      context: ./backend
      dockerfile: Dockerfile
      args:
        BASE_IMAGE: legal-ai/backend-base:cu130
    container_name: backend
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - QDRANT_URL=${QDRANT_URL}
      - QDRANT_COLLECTION=${QDRANT_COLLECTION}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
      - RAG_TOP_K=${RAG_TOP_K}
      - EMBED_DEVICE=auto
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
      # настройки стартовых проверок (выполнятся в app.startup)
      - STARTUP_CHECKS=1
      - STARTUP_CUDA_NAME=0      # имя GPU можно включать позже
      - SELF_CHECK_TIMEOUT=5   # ждать ollama/qdrant до N секунд
      - SELF_CHECK_GEN=0        # 1 = сделать тест-генерацию через ollama
      - EMBED_PRELOAD=0         # 1 = прогрузить эмбеддер на старте
      # Reranker
      - RERANK_ENABLE=1
      - RERANK_PRELOAD=0
      - RERANKER_MODEL=BAAI/bge-reranker-v2-m3
      - RERANK_DEVICE=auto          # auto|cuda|cpu
      - RERANK_KEEP=5               # сколько оставить после rerank из топ-K
      - RERANK_BATCH=16             # батч скоринга
      - RERANK_DEBUG=0              # 1 = печатать скоры в логи
      # Net-tests
      - NET_TEST_URLS=https://publication.pravo.gov.ru/,https://pravo.gov.ru/,https://docs.cntd.ru/

    depends_on:
      ollama:
        condition: service_healthy
      ollama-pull:
        condition: service_completed_successfully
      qdrant:
        condition: service_started
    volumes:
      - ./:/workspace
      # - ./.hf_cache:/root/.cache/huggingface
    working_dir: /workspace/backend
    # просто запускаем uvicorn — без shell-скриптов
    command: uvicorn app:app --host 0.0.0.0 --port 8000 --reload --log-level info
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8000/health >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 20
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    restart: unless-stopped

volumes:
  ollama:
    name: legal_ai_1c_ollama
  qdrant_data:
    name: legal_ai_1c_qdrant_data
